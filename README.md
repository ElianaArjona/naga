# ğŸ™ Naga Project â€” Streaming Data Pipeline with Airflow, Kafka, ClickHouse & PostgreSQL

---

## ğŸ§­ Project Overview

**Naga** is a modern, highly scalable streaming data pipeline architecture designed to process, transform, and store real-time data at scale. This project integrates multiple distributed systems following a microservices approach, orchestrated with Docker and deployed serverlessly on Render.

---

## ğŸ”¥ Key Components

- **Apache Airflow** â€” Workflow orchestration and DAG scheduling.
- **Apache Kafka** â€” Real-time event streaming platform and distributed message broker.
- **ClickHouse** â€” Column-oriented OLAP database for high-performance analytical queries.
- **PostgreSQL** â€” Relational database for transactional data and Airflow metadata.
- **Python Kafka Consumers** â€” Services that consume streaming data and write into PostgreSQL and ClickHouse.
- **Docker Compose** â€” Complete local development stack.
- **Render** â€” Fully managed serverless cloud deployment.

---

## ğŸŒ High-Level Architecture

```text
+----------------+      +--------------+      +-----------------+
|  API Producer  | ---> |    Kafka     | ---> |  Kafka Consumers |
+----------------+      +--------------+      +-----------------+
                                                  |           |
                                               +--+--+     +--+--+
                                               |Postgres|  |ClickHouse|
                                               +-------+   +--------+
```

- **Producers:** Airflow DAGs collect external data sources (e.g. Star Wars API, PokÃ©mon API), stream data into Kafka topics.
- **Consumers:** Python microservices consume Kafka topics and write transformed data into PostgreSQL and ClickHouse for storage and analytics.
- **Storage:** PostgreSQL for structured transactional data, ClickHouse for analytics.

---

## ğŸš€ Deployment Modes

- âœ… **Local Development:** Docker Compose powered full-stack for local development.
- âœ… **Production Deployment:** Fully serverless Render deployment using Render Blueprints (`render.yaml`).

---

## ğŸ³ Local Development Setup

### âœ… Prerequisites

- Docker installed
- Docker Compose installed
- Python 3.12 installed (optional for local scripts)

### âœ… Clone and Launch

```bash
git clone https://github.com/YOUR-ORG/naga.git
cd naga
docker-compose up --build
```

This will launch:

- Airflow Webserver (http://localhost:8080)
- Airflow Scheduler
- Kafka Broker
- Zookeeper
- ClickHouse DB
- PostgreSQL DB
- Kafka Consumers (Postgres & ClickHouse)

---

## ğŸ“‚ Local Development File Structure

```text
naga/
â”‚
â”œâ”€â”€ dags/                    # Airflow DAGs
â”œâ”€â”€ docker-compose.yml       # Local Docker Compose definition
â”œâ”€â”€ Dockerfile.airflow       # Custom Airflow Docker build
â”‚
â”œâ”€â”€ postgres_consumer/       # Postgres Kafka consumer microservice
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â””â”€â”€ consumer.py
â”‚
â”œâ”€â”€ clickhouse_consumer/     # ClickHouse Kafka consumer microservice
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â””â”€â”€ consumer.py
â”‚
â””â”€â”€ render.yaml              # Render cloud deployment blueprint
```

---

## ğŸš€ Cloud Deployment on Render

This project leverages **Render Blueprints** to fully automate infrastructure deployment via the `render.yaml` file.

### âœ… Render Deployment Steps

1ï¸âƒ£ **Connect your GitHub repo to Render**

2ï¸âƒ£ In Render, select **Blueprint Deploy** and pick your branch.

3ï¸âƒ£ Render will automatically deploy:

- Airflow Webserver (Web service)
- Airflow Scheduler (Worker)
- Postgres Consumer (Worker)
- ClickHouse Consumer (Worker)
- Kafka (Worker using public Docker image)
- Zookeeper (Worker using public Docker image)
- ClickHouse (Worker using public Docker image)

4ï¸âƒ£ âœ… Create Managed PostgreSQL instance via Render UI:
- Database Name: `airflow`
- Username: `airflow`
- Password: `airflow` (or a strong password)
- Internal Hostname: e.g. `postgres-db:5432` for Render DNS

5ï¸âƒ£ âœ… Create Environment Group (`.env`) in Render with the following environment variables:

| Variable | Value |
|----------|-------|
| POSTGRES_USER | airflow |
| POSTGRES_PASSWORD | airflow |
| POSTGRES_DB | airflow |
| CLICKHOUSE_DB | default |
| CLICKHOUSE_USER | default |
| CLICKHOUSE_PASSWORD | admin |
| KAFKA_BROKER_ID | 1 |
| AIRFLOW_USER | admin |
| AIRFLOW_PASSWORD | admin |
| AIRFLOW_EMAIL | admin@example.com |
| AIRFLOW__CORE__FERNET_KEY | (generate one: `python -c "from cryptography.fernet import Fernet; print(Fernet.generate_key().decode())"`) |
| AIRFLOW__CORE__SQL_ALCHEMY_CONN | postgresql+psycopg2://airflow:airflow@postgres-db:5432/airflow |

---

## âš™ï¸ Docker Build Contexts Summary

| Service | Build Context | Dockerfile |
|---------|----------------|------------|
| Airflow Webserver | `./` | `Dockerfile.airflow` |
| Airflow Scheduler | `./` | `Dockerfile.airflow` |
| Postgres Consumer | `./postgres_consumer/` | `Dockerfile` |
| ClickHouse Consumer | `./clickhouse_consumer/` | `Dockerfile` |

---

## ğŸ“¡ Render Internal DNS Communication

| Service | Internal DNS |
|---------|---------------|
| Kafka | `kafka:9092` |
| Zookeeper | `zookeeper:2181` |
| ClickHouse | `clickhouse:8123` |
| Postgres | `postgres-db:5432` |

Render manages DNS resolution automatically between services.

---

## ğŸ— render.yaml Summary

The `render.yaml` defines full infrastructure-as-code deployment:

- âœ… 1x Web Service (Airflow Webserver)
- âœ… 6x Workers (Scheduler, Consumers, Kafka, Zookeeper, ClickHouse)

All services are independently scalable and isolated.

---

## ğŸ§ª Testing Kafka Consumers

- Data is pushed into Kafka topics:
  - `pokemon-data`
  - `pokemon-data-by-type`
- Kafka consumers automatically consume, transform and write data into:
  - PostgreSQL
  - ClickHouse

You can manually trigger DAGs in Airflow UI for data ingestion.

---

## ğŸ· Design Goals

- Fully event-driven
- Microservices architecture
- Streaming-first design
- Stateless deployment (except data layers)
- Cloud-native & scalable
- Serverless Render deployment (no dedicated DevOps needed)

---

## âš ï¸ Known Limitations

- Kafka partitioning is managed in producer logic based on PokÃ©mon type.
- Managed PostgreSQL must be manually provisioned in Render UI.
- Proper resource sizing depends on production data load.
- Internal DNS names must be used between services.

---

## ğŸš€ CI/CD Deployment Flow

- âœ… All deployment is automated through Render Blueprints.
- âœ… Any commit to the repo auto-triggers new deployments.
- âœ… Secrets and configurations are securely managed via Render Environment Groups.

---

## ğŸ“„ License

This project is licensed for educational & prototyping purposes.

---

**Enjoy your fully distributed streaming microservices pipeline â€” cloud-native and fully serverless. ğŸš€**

